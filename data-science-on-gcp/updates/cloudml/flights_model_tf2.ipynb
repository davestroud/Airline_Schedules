{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing, Training, and Deploying a TensorFlow model on Google Cloud Platform (completely within Jupyter)\n",
    "\n",
    "\n",
    "In Chapter 9 of [Data Science on the Google Cloud Platform](http://shop.oreilly.com/product/0636920057628.do), I trained a TensorFlow Estimator model to predict flight delays.\n",
    "\n",
    "In this notebook, we'll modernize the workflow:\n",
    "* Use eager mode for TensorFlow development\n",
    "* Use tf.data to write the input pipeline\n",
    "* Run the notebook as-is on Cloud using Deep Learning VM or Kubeflow pipelines\n",
    "* Deploy the trained model to Cloud ML Engine as a web service\n",
    "\n",
    "The combination of eager mode, tf.data and DLVM/KFP makes this workflow a lot easier.\n",
    "We don't need to deal with Python packages or Docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "# In \"production\", these will be replaced by the parameters passed to papermill\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'\n",
    "DEVELOP_MODE = True\n",
    "NBUCKETS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the input data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET = \"gs://cloud-training-demos/flights/chapter8/output/\"\n",
    "TRAIN_DATA_PATTERN = DATA_BUCKET + \"train*\"\n",
    "VALID_DATA_PATTERN = DATA_BUCKET + \"test*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos/flights/chapter8/output/delays.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/testFlights-00006-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00000-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00001-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00002-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00003-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00004-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00005-of-00007.csv\n",
      "gs://cloud-training-demos/flights/chapter8/output/trainFlights-00006-of-00007.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def load_dataset(pattern):\n",
    "  return tf.data.experimental.make_csv_dataset(pattern, 1, CSV_COLUMNS, DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ontime': array([0.], dtype=float32), 'dep_delay': array([34.], dtype=float32), 'taxiout': array([17.], dtype=float32), 'distance': array([453.], dtype=float32), 'avg_dep_delay': array([26.245787], dtype=float32), 'avg_arr_delay': array([6.], dtype=float32), 'carrier': array([b'DL'], dtype=object), 'dep_lat': array([33.636665], dtype=float32), 'dep_lon': array([-84.42778], dtype=float32), 'arr_lat': array([34.729443], dtype=float32), 'arr_lon': array([-92.224724], dtype=float32), 'origin': array([b'ATL'], dtype=object), 'dest': array([b'LIT'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-1.], dtype=float32), 'taxiout': array([20.], dtype=float32), 'distance': array([453.], dtype=float32), 'avg_dep_delay': array([26.245787], dtype=float32), 'avg_arr_delay': array([-6.], dtype=float32), 'carrier': array([b'DL'], dtype=object), 'dep_lat': array([33.636665], dtype=float32), 'dep_lon': array([-84.42778], dtype=float32), 'arr_lat': array([34.729443], dtype=float32), 'arr_lon': array([-92.224724], dtype=float32), 'origin': array([b'ATL'], dtype=object), 'dest': array([b'LIT'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-3.], dtype=float32), 'taxiout': array([24.], dtype=float32), 'distance': array([552.], dtype=float32), 'avg_dep_delay': array([32.05482], dtype=float32), 'avg_arr_delay': array([3.], dtype=float32), 'carrier': array([b'EV'], dtype=object), 'dep_lat': array([41.979443], dtype=float32), 'dep_lon': array([-87.9075], dtype=float32), 'arr_lat': array([34.729443], dtype=float32), 'arr_lon': array([-92.224724], dtype=float32), 'origin': array([b'ORD'], dtype=object), 'dest': array([b'LIT'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-9.], dtype=float32), 'taxiout': array([11.], dtype=float32), 'distance': array([748.], dtype=float32), 'avg_dep_delay': array([25.050283], dtype=float32), 'avg_arr_delay': array([-18.], dtype=float32), 'carrier': array([b'OO'], dtype=object), 'dep_lat': array([33.9425], dtype=float32), 'dep_lon': array([-118.40806], dtype=float32), 'arr_lat': array([44.124443], dtype=float32), 'arr_lon': array([-123.211945], dtype=float32), 'origin': array([b'LAX'], dtype=object), 'dest': array([b'EUG'], dtype=object)}\n",
      "{'ontime': array([1.], dtype=float32), 'dep_delay': array([-1.], dtype=float32), 'taxiout': array([22.], dtype=float32), 'distance': array([1294.], dtype=float32), 'avg_dep_delay': array([26.217432], dtype=float32), 'avg_arr_delay': array([-2.], dtype=float32), 'carrier': array([b'WN'], dtype=object), 'dep_lat': array([36.08], dtype=float32), 'dep_lon': array([-115.15222], dtype=float32), 'arr_lat': array([34.729443], dtype=float32), 'arr_lon': array([-92.224724], dtype=float32), 'origin': array([b'LAS'], dtype=object), 'dest': array([b'LIT'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "if DEVELOP_MODE:\n",
    "    dataset = load_dataset(TRAIN_DATA_PATTERN)\n",
    "    for n, data in enumerate(dataset):\n",
    "        numpy_data = {k: v.numpy() for k, v in data.items()} # .numpy() works only in eager mode\n",
    "        print(numpy_data)\n",
    "        if n>3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example_input.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile example_input.json\n",
    "{\"dep_delay\": 14.0, \"taxiout\": 13.0, \"distance\": 319.0, \"avg_dep_delay\": 25.863039, \"avg_arr_delay\": 27.0, \"carrier\": \"WN\", \"dep_lat\": 32.84722, \"dep_lon\": -96.85167, \"arr_lat\": 31.9425, \"arr_lon\": -102.20194, \"origin\": \"DAL\", \"dest\": \"MAF\"}\n",
    "{\"dep_delay\": -9.0, \"taxiout\": 21.0, \"distance\": 301.0, \"avg_dep_delay\": 41.050808, \"avg_arr_delay\": -7.0, \"carrier\": \"EV\", \"dep_lat\": 29.984444, \"dep_lon\": -95.34139, \"arr_lat\": 27.544167, \"arr_lon\": -99.46167, \"origin\": \"IAH\", \"dest\": \"LRD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling prepare\n",
      "[(OrderedDict([('dep_delay', <tf.Tensor: id=345, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-3.],\n",
      "       [-6.],\n",
      "       [-3.],\n",
      "       [99.],\n",
      "       [-3.]], dtype=float32)>), ('taxiout', <tf.Tensor: id=351, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[10.],\n",
      "       [ 9.],\n",
      "       [12.],\n",
      "       [19.],\n",
      "       [20.]], dtype=float32)>), ('distance', <tf.Tensor: id=349, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[ 399.],\n",
      "       [ 519.],\n",
      "       [ 511.],\n",
      "       [1437.],\n",
      "       [1437.]], dtype=float32)>), ('avg_dep_delay', <tf.Tensor: id=343, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[22.600828],\n",
      "       [26.191599],\n",
      "       [24.076136],\n",
      "       [30.699594],\n",
      "       [37.17035 ]], dtype=float32)>), ('avg_arr_delay', <tf.Tensor: id=342, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-14.5 ],\n",
      "       [-14.75],\n",
      "       [ -1.  ],\n",
      "       [ 23.75],\n",
      "       [-10.  ]], dtype=float32)>), ('carrier', <tf.Tensor: id=344, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'OO'],\n",
      "       [b'WN'],\n",
      "       [b'WN'],\n",
      "       [b'OO'],\n",
      "       [b'OO']], dtype=object)>), ('dep_lat', <tf.Tensor: id=346, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[47.45    ],\n",
      "       [36.08    ],\n",
      "       [37.72278 ],\n",
      "       [41.979443],\n",
      "       [41.977222]], dtype=float32)>), ('dep_lon', <tf.Tensor: id=347, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-122.31167],\n",
      "       [-115.15222],\n",
      "       [-122.22139],\n",
      "       [ -87.9075 ],\n",
      "       [ -87.90806]], dtype=float32)>), ('arr_lat', <tf.Tensor: id=340, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[43.564445],\n",
      "       [43.564445],\n",
      "       [43.564445],\n",
      "       [43.564445],\n",
      "       [43.564445]], dtype=float32)>), ('arr_lon', <tf.Tensor: id=341, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[-116.22278],\n",
      "       [-116.22278],\n",
      "       [-116.22278],\n",
      "       [-116.22278],\n",
      "       [-116.22278]], dtype=float32)>), ('origin', <tf.Tensor: id=350, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'SEA'],\n",
      "       [b'LAS'],\n",
      "       [b'OAK'],\n",
      "       [b'ORD'],\n",
      "       [b'ORD']], dtype=object)>), ('dest', <tf.Tensor: id=348, shape=(5, 1), dtype=string, numpy=\n",
      "array([[b'BOI'],\n",
      "       [b'BOI'],\n",
      "       [b'BOI'],\n",
      "       [b'BOI'],\n",
      "       [b'BOI']], dtype=object)>)]), <tf.Tensor: id=352, shape=(5, 1), dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.]], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "def features_and_labels(features):\n",
    "  label = features.pop('ontime') # this is what we will train for\n",
    "  return features, label\n",
    "\n",
    "def prepare_dataset(pattern, batch_size, truncate=None, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "  dataset = load_dataset(pattern)\n",
    "  dataset = dataset.map(features_and_labels)\n",
    "  dataset = dataset.cache()\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(1)\n",
    "  if truncate is not None:\n",
    "    dataset = dataset.take(truncate)\n",
    "  return dataset\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(\"Calling prepare\")\n",
    "    one_item = prepare_dataset(TRAIN_DATA_PATTERN, batch_size=5, truncate=1)\n",
    "    print(list(one_item)) # should print one batch of 2 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow wide-and-deep model\n",
    "\n",
    "We'll create feature columns, and do some discretization and feature engineering.\n",
    "See the book for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.feature_column as fc\n",
    "\n",
    "real = {\n",
    "    colname : fc.numeric_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "}\n",
    "sparse = {\n",
    "      'carrier': fc.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "      'origin' : fc.categorical_column_with_hash_bucket('origin', hash_bucket_size=1000),\n",
    "      'dest'   : fc.categorical_column_with_hash_bucket('dest', hash_bucket_size=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['carrier', 'origin', 'dest', 'dep_loc', 'arr_loc', 'dep_arr', 'ori_dest'])\n",
      "dict_keys(['dep_delay', 'taxiout', 'distance', 'avg_dep_delay', 'avg_arr_delay', 'dep_lat', 'dep_lon', 'arr_lat', 'arr_lon', 'carrier', 'origin', 'dest', 'dep_loc', 'arr_loc', 'dep_arr', 'ori_dest'])\n"
     ]
    }
   ],
   "source": [
    "latbuckets = np.linspace(20.0, 50.0, NBUCKETS).tolist()  # USA\n",
    "lonbuckets = np.linspace(-120.0, -70.0, NBUCKETS).tolist() # USA\n",
    "disc = {}\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "})\n",
    "disc.update({\n",
    "       'd_{}'.format(key) : fc.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "})\n",
    "\n",
    "# cross columns that make sense in combination\n",
    "sparse['dep_loc'] = fc.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['arr_loc'] = fc.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']], NBUCKETS*NBUCKETS)\n",
    "sparse['dep_arr'] = fc.crossed_column([sparse['dep_loc'], sparse['arr_loc']], NBUCKETS ** 4)\n",
    "sparse['ori_dest'] = fc.crossed_column(['origin', 'dest'], hash_bucket_size=1000)\n",
    "\n",
    "# embed all the sparse columns\n",
    "embed = {\n",
    "       colname : fc.embedding_column(col, 10) \\\n",
    "          for colname, col in sparse.items()\n",
    "}\n",
    "real.update(embed)\n",
    "\n",
    "if DEVELOP_MODE:\n",
    "    print(sparse.keys())\n",
    "    print(real.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving\n",
    "\n",
    "This serving input function is how the model will be deployed for prediction. We require these fields for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import placeholder\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        # All the real-valued columns\n",
    "        column: placeholder(tf.float32, [None]) \\\n",
    "             for column in ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \n",
    "                            ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    feature_placeholders.update({\n",
    "        column: placeholder(tf.string, [None]) for column in ['carrier', 'origin', 'dest']\n",
    "    })\n",
    "    features = feature_placeholders # no transformations\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and evaluate once in a while\n",
    "\n",
    "Also checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trained model to gs://cloud-training-demos-ml/flights/trained_model\n"
     ]
    }
   ],
   "source": [
    "model_dir='gs://{}/flights/trained_model'.format(BUCKET)\n",
    "os.environ['OUTDIR'] = model_dir  # needed for deployment\n",
    "print('Writing trained model to {}'.format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://cloud-training-demos-ml/flights/trained_model/checkpoint#1553294072663821...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/eval/events.out.tfevents.1553293993.laktfnightly#1553294081019312...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/export/#1553294082698379...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/export/exporter/temp-b'1553294081'/#1553294083651837...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/export/exporter/#1553294083180243...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/graph.pbtxt#1553294056478447...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-10.index#1553294071477769...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-0.data-00001-of-00002#1553294060756692...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-10.data-00000-of-00002#1553294070951705...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-0.index#1553294061607443...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-0.data-00000-of-00002#1553294061220583...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-10.data-00001-of-00002#1553294070501091...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-0.meta#1553294063936178...\n",
      "Removing gs://cloud-training-demos-ml/flights/trained_model/model.ckpt-10.meta#1553294073788613...\n",
      "/ [14/14 objects] 100% Done                                                     \n",
      "Operation completed over 14 objects.                                             \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -rf $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0322 22:37:58.388895 140055668778368 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0322 22:37:58.418073 140055668778368 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0322 22:38:07.437546 140055668778368 deprecation.py:323] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0322 22:38:07.439567 140055668778368 tf_logging.py:161] Export includes no default signature!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.785,\n",
       "  'accuracy_baseline': 0.824,\n",
       "  'auc': 0.84984213,\n",
       "  'auc_precision_recall': 0.9575325,\n",
       "  'average_loss': 0.6758764,\n",
       "  'label/mean': 0.824,\n",
       "  'loss': 67.58764,\n",
       "  'precision': 0.93191487,\n",
       "  'prediction/mean': 0.68610954,\n",
       "  'recall': 0.7973301,\n",
       "  'global_step': 10},\n",
       " [b'gs://cloud-training-demos-ml/flights/trained_model/export/exporter/1553294283'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = tf.compat.v1.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir = model_dir,\n",
    "        linear_feature_columns = sparse.values(),\n",
    "        dnn_feature_columns = real.values(),\n",
    "        dnn_hidden_units = [64, 32])\n",
    "\n",
    "train_batch_size = 64\n",
    "train_input_fn = lambda: prepare_dataset(TRAIN_DATA_PATTERN, train_batch_size)\n",
    "eval_batch_size = 100 if DEVELOP_MODE else 10000\n",
    "eval_input_fn = lambda: prepare_dataset(VALID_DATA_PATTERN, eval_batch_size, eval_batch_size*10, tf.estimator.ModeKeys.EVAL)\n",
    "num_steps = 10 if DEVELOP_MODE else (1000000 // train_batch_size)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps = num_steps)\n",
    "exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input_fn, steps=10, exporters=exporter)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-training-demos-ml/flights/trained_model/export/exporter/1553294283/\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['arr_lat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_7:0\n",
      "    inputs['arr_lon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_8:0\n",
      "    inputs['avg_arr_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_4:0\n",
      "    inputs['avg_dep_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_3:0\n",
      "    inputs['carrier'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_9:0\n",
      "    inputs['dep_delay'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder:0\n",
      "    inputs['dep_lat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_5:0\n",
      "    inputs['dep_lon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_6:0\n",
      "    inputs['dest'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_11:0\n",
      "    inputs['distance'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_2:0\n",
      "    inputs['origin'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: Placeholder_10:0\n",
      "    inputs['taxiout'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/ExpandDims:0\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/str_classes:0\n",
      "    outputs['logistic'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/logistic:0\n",
      "    outputs['logits'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: add:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 2)\n",
      "        name: head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo $model_dir\n",
    "saved_model_cli show --dir ${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"flights\"\n",
    "MODEL_VERSION=\"kfp\"\n",
    "TFVERSION=\"2.0\"\n",
    "MODEL_LOCATION=$(gsutil ls ${OUTDIR}/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#yes | gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine predict --model=flights --version=kfp --json-instances=example_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
